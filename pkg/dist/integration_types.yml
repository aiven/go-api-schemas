autoscaler:
  title: Integration user config
  type: object
autoscaler_service:
  title: Service autoscaler user config
  type: object
  properties:
    autoscaling:
      title: ServiceAutoscalerIntegrationUserConfig
      description: Configure service autoscaling integration. This is an experimental api and subject to change. For further assistance, contact the Aiven support team mailto:support@aiven.io or your account team.
      type: object
      properties:
        max_plan:
          title: Max Plan
          type: string
          min_length: 3
          max_length: 64
          example: business-5
        min_plan:
          title: Min Plan
          type: string
          min_length: 3
          max_length: 64
          example: business-4
clickhouse_credentials:
  title: Integration user config
  type: object
  properties:
    grants:
      title: Grants to assign.
      type: array
      items:
        title: ClickHouse grant
        type: object
        required:
          - user
        properties:
          user:
            title: User or role to assign the grant to
            type: string
            min_length: 1
            max_length: 64
            example: alice
      max_items: 64
clickhouse_kafka:
  title: Integration user config
  type: object
  properties:
    tables:
      title: Tables to create
      description: Array of table configurations that define how Kafka topics are mapped to ClickHouse tables. Each table configuration specifies the table structure, associated Kafka topics, and read/write settings.
      type: array
      default: []
      items:
        title: Table to create
        type: object
        required:
          - name
          - columns
          - topics
          - data_format
          - group_name
        properties:
          auto_offset_reset:
            title: Action to take when there is no initial offset in offset store or the desired offset is out of range
            description: Determines where to start reading from Kafka when no offset is stored or the stored offset is out of range. 'earliest' starts from the beginning, 'latest' starts from the end.
            type: string
            default: earliest
            enum:
              - value: beginning
              - value: earliest
              - value: end
              - value: largest
              - value: latest
              - value: smallest
            example: latest
          columns:
            title: Table columns
            description: Array of column definitions that specify the structure of the ClickHouse table. Each column maps to a field in the Kafka messages.
            type: array
            items:
              title: Table column
              type: object
              required:
                - name
                - type
              properties:
                name:
                  title: Column name
                  description: The name of the column in the ClickHouse table. This should match the field names in your Kafka message format.
                  type: string
                  min_length: 1
                  max_length: 40
                  example: key
                type:
                  title: Column type
                  description: The ClickHouse data type for this column. Must be a valid ClickHouse data type that can handle the data format.
                  type: string
                  min_length: 1
                  max_length: 1000
                  example: UInt64
            max_items: 100
          data_format:
            title: Message data format
            description: The format of the messages in the Kafka topics. Determines how ClickHouse parses and serializes the data (e.g., JSON, CSV, Avro).
            type: string
            default: JSONEachRow
            enum:
              - value: Avro
              - value: AvroConfluent
              - value: CSV
              - value: JSONAsString
              - value: JSONCompactEachRow
              - value: JSONCompactStringsEachRow
              - value: JSONEachRow
              - value: JSONStringsEachRow
              - value: MsgPack
              - value: Parquet
              - value: RawBLOB
              - value: TSKV
              - value: TSV
              - value: TabSeparated
            example: JSONEachRow
          date_time_input_format:
            title: Method to read DateTime from text input formats
            description: Specifies how ClickHouse should parse DateTime values from text-based input formats. 'basic' uses simple parsing, 'best_effort' attempts more flexible parsing.
            type: string
            default: basic
            enum:
              - value: basic
              - value: best_effort
              - value: best_effort_us
            example: best_effort
          group_name:
            title: Kafka consumers group
            description: The Kafka consumer group name. Multiple consumers with the same group name will share the workload and maintain offset positions.
            type: string
            default: clickhouse
            min_length: 1
            max_length: 249
            example: clickhouse
          handle_error_mode:
            title: How to handle errors for Kafka engine
            description: Defines how ClickHouse should handle errors when processing Kafka messages. 'default' stops on errors, 'stream' continues processing and logs errors.
            type: string
            default: default
            enum:
              - value: default
              - value: stream
            example: stream
          max_block_size:
            title: Number of row collected by poll(s) for flushing data from Kafka
            description: Maximum number of rows to collect before flushing data between Kafka and ClickHouse.
            type: integer
            default: "0"
            minimum: 0
            maximum: 1e+09
            example: "100000"
          max_rows_per_message:
            title: The maximum number of rows produced in one kafka message for row-based formats
            description: Maximum number of rows that can be processed from a single Kafka message for row-based formats. Useful for controlling memory usage.
            type: integer
            default: "1"
            minimum: 1
            maximum: 1e+09
            example: "100000"
          name:
            title: Name of the table
            description: The name of the ClickHouse table to be created. This table can consume data from and write data to the specified Kafka topics.
            type: string
            min_length: 1
            max_length: 40
            example: events
          num_consumers:
            title: The number of consumers per table per replica
            description: Number of Kafka consumers to run per table per replica. Increasing this can improve throughput but may increase resource usage.
            type: integer
            default: "1"
            minimum: 1
            maximum: 10
            example: "4"
          poll_max_batch_size:
            title: Maximum amount of messages to be polled in a single Kafka poll
            description: Maximum number of messages to fetch in a single Kafka poll operation for reading.
            type: integer
            default: "0"
            minimum: 0
            maximum: 1e+09
            example: "10000"
          poll_max_timeout_ms:
            title: Timeout in milliseconds for a single poll from Kafka. Takes the value of the stream_flush_interval_ms server setting by default (500ms).
            description: Maximum time to wait for messages during a Kafka poll operation for reading.
            type: integer
            default: "0"
            minimum: 0
            maximum: 30000
            example: "1000"
          producer_batch_num_messages:
            title: The maximum number of messages in a batch sent to Kafka
            description: The maximum number of messages in a batch sent to Kafka. If the number of messages exceeds this value, the batch is sent.
            type: integer
            default: "10000"
            minimum: 1
            maximum: 1e+06
            example: "10000"
          producer_batch_size:
            title: The maximum size of a batch of messages sent to Kafka
            description: The maximum size in bytes of a batch of messages sent to Kafka. If the batch size is exceeded, the batch is sent.
            type: integer
            default: "1000000"
            minimum: 0
            maximum: 2.147483647e+09
            example: "1000000"
          producer_compression_codec:
            title: The compression codec to use for Kafka producer
            description: The compression codec to use when sending a batch of messages to Kafka.
            type: string
            default: none
            enum:
              - value: gzip
              - value: lz4
              - value: none
              - value: snappy
              - value: zstd
            example: zstd
          producer_compression_level:
            title: The compression level to use for Kafka producer
            description: 'The compression level to use when sending a batch of messages to Kafka. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level.'
            type: integer
            default: "-1"
            minimum: -1
            maximum: 12
            example: "5"
          producer_linger_ms:
            title: The time to wait for additional messages before sending a batch
            description: The time in milliseconds to wait for additional messages before sending a batch. If the time is exceeded, the batch is sent.
            type: integer
            default: "5"
            minimum: 0
            maximum: 900000
            example: "5"
          producer_queue_buffering_max_kbytes:
            title: The maximum size of the buffer in kilobytes before sending
            description: The maximum size of the producer queue in kilobytes.
            type: integer
            default: "1048576"
            minimum: 0
            maximum: 2.147483647e+09
            example: "1048576"
          producer_queue_buffering_max_messages:
            title: The maximum number of messages to buffer before sending
            description: The maximum number of messages in the producer queue.
            type: integer
            default: "100000"
            minimum: 0
            maximum: 2.147483647e+09
            example: "100000"
          producer_request_required_acks:
            title: The number of acknowledgments required from Kafka brokers for a message to be considered successful
            description: 'The number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: 0=Broker does not send any response/ack to client, -1 will block until message is committed by all in sync replicas (ISRs).'
            type: integer
            default: "-1"
            minimum: -1
            maximum: 1000
            example: "1"
          skip_broken_messages:
            title: Skip at least this number of broken messages from Kafka topic per block
            description: Number of broken messages to skip before stopping processing when reading from Kafka. Useful for handling corrupted data without failing the entire integration.
            type: integer
            default: "0"
            minimum: 0
            maximum: 1e+09
            example: "10000"
          thread_per_consumer:
            title: Provide an independent thread for each consumer. All consumers run in the same thread by default.
            description: When enabled, each consumer runs in its own thread, providing better isolation and potentially better performance for high-throughput scenarios.
            type: boolean
            default: false
            example: true
          topics:
            title: Kafka topics
            description: Array of Kafka topics that this table will read data from or write data to. Messages from all specified topics will be inserted into this table, and data inserted into this table will be published to the topics.
            type: array
            items:
              title: Kafka topic
              type: object
              required:
                - name
              properties:
                name:
                  title: Name of the topic
                  description: The name of the Kafka topic to read messages from or write messages to. The topic must exist in the Kafka cluster.
                  type: string
                  min_length: 1
                  max_length: 249
                  pattern: ^(?!\.$|\.\.$)[-_.A-Za-z0-9]+$
                  example: topic_name
                  user_error: Must consist of alpha-numeric characters, underscores, dashes or dots, max 249 characters
            max_items: 100
      max_items: 400
clickhouse_postgresql:
  title: Integration user config
  type: object
  properties:
    databases:
      title: Databases to expose
      type: array
      default:
        - database: defaultdb
          schema: public
      items:
        title: Database to expose
        type: object
        properties:
          database:
            title: PostgreSQL database to expose
            type: string
            default: defaultdb
            min_length: 1
            max_length: 63
            example: defaultdb
          schema:
            title: PostgreSQL schema to expose
            type: string
            default: public
            min_length: 1
            max_length: 63
            example: public
      max_items: 10
dashboard:
  title: Integration user config
  type: object
datadog:
  type: object
  properties:
    datadog_dbm_enabled:
      title: Enable Datadog Database Monitoring
      type: boolean
      example: true
    datadog_pgbouncer_enabled:
      title: Enable Datadog PgBouncer Metric Tracking
      type: boolean
      example: true
    datadog_tags:
      title: Custom tags provided by user
      type: array
      items:
        title: Datadog tag defined by user
        type: object
        required:
          - tag
        properties:
          comment:
            title: Optional tag explanation
            type: string
            max_length: 1024
            example: Used to tag primary replica metrics
          tag:
            title: Tag value
            description: 'Tag format and usage are described here: https://docs.datadoghq.com/getting_started/tagging. Tags with prefix ''aiven-'' are reserved for Aiven.'
            type: string
            min_length: 1
            max_length: 200
            pattern: ^(?!aiven-)[^\W\d_](?:[:\w./-]*[\w./-])?$
            example: replica:primary
            user_error: |
              Tags must start with a letter and after that may contain the characters listed below:
              alphanumerics, underscores, minuses, colons, periods, slashes.
              A tag cannot end with a colon.
              Tags can be up to 200 characters long and support Unicode.
              Tags with prefix 'aiven-' are reserved for Aiven.
              More info: https://docs.datadoghq.com/getting_started/tagging.
      max_items: 32
      example:
        - tag: foo
        - comment: Useful tag
          tag: bar:buzz
    exclude_consumer_groups:
      title: List of custom metrics
      type: array
      items:
        title: Consumer groups to exclude
        type: string
        max_length: 1024
        example: '[ group_a, group_b ]'
      max_items: 1024
    exclude_topics:
      title: List of topics to exclude
      type: array
      items:
        title: Topics to exclude
        type: string
        max_length: 1024
        example: '[ topic_x, topic_y ]'
      max_items: 1024
    include_consumer_groups:
      title: List of custom metrics
      type: array
      items:
        title: Consumer groups to include
        type: string
        max_length: 1024
        example: '[ group_a, group_b ]'
      max_items: 1024
    include_topics:
      title: List of topics to include
      type: array
      items:
        title: Topics to include
        type: string
        max_length: 1024
        example: '[ topic_x, topic_y ]'
      max_items: 1024
    kafka_custom_metrics:
      title: List of custom metrics
      type: array
      items:
        title: Metric name
        type: string
        enum:
          - value: kafka.log.log_end_offset
          - value: kafka.log.log_size
          - value: kafka.log.log_start_offset
        max_length: 1024
        example: kafka.log.log_size
      max_items: 1024
    max_jmx_metrics:
      title: Maximum number of JMX metrics to send
      type: integer
      minimum: 10
      maximum: 100000
      example: "2000"
    mirrormaker_custom_metrics:
      title: List of custom metrics
      type: array
      items:
        title: Metric name
        type: string
        enum:
          - value: kafka_mirrormaker_summary.replication_lag
        max_length: 1024
        example: kafka_mirrormaker_summary.replication_lag
      max_items: 1024
    opensearch:
      title: Datadog Opensearch Options
      type: object
      properties:
        cluster_stats_enabled:
          title: Enable Datadog Opensearch Cluster Monitoring
          type: boolean
          example: true
        index_stats_enabled:
          title: Enable Datadog Opensearch Index Monitoring
          type: boolean
          example: true
        pending_task_stats_enabled:
          title: Enable Datadog Opensearch Pending Task Monitoring
          type: boolean
          example: true
        pshard_stats_enabled:
          title: Enable Datadog Opensearch Primary Shard Monitoring
          type: boolean
          example: true
    redis:
      title: Datadog Redis Options
      type: object
      properties:
        command_stats_enabled:
          title: Redis command statistics for Datadog integrations
          description: Enable command_stats option in the agent's configuration
          type: boolean
          default: false
datasource:
  title: Integration user config
  type: object
disaster_recovery:
  title: Integration user config
  type: object
external_aws_cloudwatch_logs:
  title: Integration user config
  type: object
  properties:
    selected_log_fields:
      title: List of logging fields to include
      description: The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
      type: array
      items:
        title: Log field name
        type: string
        enum:
          - value: HOSTNAME
          - value: PRIORITY
          - value: REALTIME_TIMESTAMP
          - value: SYSTEMD_UNIT
          - value: service_name
      max_items: 5
external_aws_cloudwatch_metrics:
  title: External AWS CloudWatch Metrics integration user config
  type: object
  properties:
    dropped_metrics:
      title: Metrics to not send to AWS CloudWatch (takes precedence over extra_metrics)
      type: array
      items:
        title: Metric name and subfield
        type: object
        required:
          - metric
          - field
        properties:
          field:
            title: Identifier of a value in the metric
            type: string
            max_length: 1000
            example: used
          metric:
            title: Identifier of the metric
            type: string
            max_length: 1000
            example: java.lang:Memory
      max_items: 1024
      example:
        - field: evicted_keys
          metric: redis
    extra_metrics:
      title: Metrics to allow through to AWS CloudWatch (in addition to default metrics)
      type: array
      items:
        title: Metric name and subfield
        type: object
        required:
          - metric
          - field
        properties:
          field:
            title: Identifier of a value in the metric
            type: string
            max_length: 1000
            example: used
          metric:
            title: Identifier of the metric
            type: string
            max_length: 1000
            example: java.lang:Memory
      max_items: 1024
      example:
        - field: evicted_keys
          metric: redis
external_elasticsearch_logs:
  title: Integration user config
  type: object
  properties:
    selected_log_fields:
      title: List of logging fields to include
      description: The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
      type: array
      items:
        title: Log field name
        type: string
        enum:
          - value: HOSTNAME
          - value: PRIORITY
          - value: REALTIME_TIMESTAMP
          - value: SYSTEMD_UNIT
          - value: service_name
      max_items: 5
external_google_cloud_logging:
  title: Integration user config
  type: object
external_opensearch_logs:
  title: Integration user config
  type: object
  properties:
    selected_log_fields:
      title: List of logging fields to include
      description: The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
      type: array
      items:
        title: Log field name
        type: string
        enum:
          - value: HOSTNAME
          - value: PRIORITY
          - value: REALTIME_TIMESTAMP
          - value: SYSTEMD_UNIT
          - value: service_name
      max_items: 5
flink:
  title: Integration user config
  type: object
flink_external_bigquery:
  title: Integration user config
  type: object
flink_external_kafka:
  title: Integration user config
  type: object
flink_external_postgresql:
  title: Integration user config
  type: object
  properties:
    stringtype:
      title: The parameter stringtype in the JDBC URL
      description: If stringtype is set to unspecified, parameters will be sent to the server as untyped values
      type: string
      enum:
        - value: unspecified
      example: unspecified
jolokia:
  title: Integration user config
  type: object
kafka_connect:
  title: Integration user config
  type: object
  properties:
    kafka_connect:
      title: Kafka Connect service configuration values
      type: object
      properties:
        config_storage_topic:
          title: The name of the topic where connector and task configuration data are stored.This must be the same for all workers with the same group_id.
          type: string
          max_length: 249
          example: __connect_configs
        group_id:
          title: A unique string that identifies the Connect cluster group this worker belongs to.
          type: string
          max_length: 249
          example: connect
        offset_storage_topic:
          title: The name of the topic where connector and task configuration offsets are stored.This must be the same for all workers with the same group_id.
          type: string
          max_length: 249
          example: __connect_offsets
        status_storage_topic:
          title: The name of the topic where connector and task configuration status updates are stored.This must be the same for all workers with the same group_id.
          type: string
          max_length: 249
          example: __connect_status
kafka_connect_postgresql:
  title: Integration user config
  type: object
kafka_logs:
  type: object
  required:
    - kafka_topic
  properties:
    kafka_topic:
      title: Topic name
      type: string
      min_length: 1
      max_length: 249
      pattern: ^(?!\.$|\.\.$)[-_.A-Za-z0-9]+$
      example: mytopic
      user_error: Must consist of alpha-numeric characters, underscores, dashes or dots, max 249 characters
    selected_log_fields:
      title: List of logging fields to include
      description: The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
      type: array
      items:
        title: Log field name
        type: string
        enum:
          - value: HOSTNAME
          - value: PRIORITY
          - value: REALTIME_TIMESTAMP
          - value: SYSTEMD_UNIT
          - value: service_name
      max_items: 5
kafka_mirrormaker:
  title: Integration user config
  type: object
  properties:
    cluster_alias:
      title: Kafka cluster alias
      description: 'The alias under which the Kafka cluster is known to MirrorMaker. Can contain the following symbols: ASCII alphanumerics, ''.'', ''_'', and ''-''.'
      type: string
      max_length: 128
      pattern: ^[a-zA-Z0-9_.-]+$
      example: kafka-abc
      user_error: Must consist of alpha-numeric ASCII characters, dashes, underscores, and dots, max 128 characters.
    kafka_mirrormaker:
      title: Kafka MirrorMaker configuration values
      type: object
      properties:
        consumer_auto_offset_reset:
          title: Set the auto.offset.reset to consumer.
          description: 'Set where consumer starts to consume data. Value `earliest`: Start replication from the earliest offset. Value `latest`: Start replication from the latest offset. Default is `earliest`.'
          type: string
          enum:
            - value: earliest
            - value: latest
        consumer_fetch_min_bytes:
          title: consumer.fetch.min.bytes
          description: The minimum amount of data the server should return for a fetch request
          type: integer
          minimum: 1
          maximum: 5.24288e+06
          example: "1024"
        consumer_max_poll_records:
          title: Set the max.poll.records to consumer.
          description: Set consumer max.poll.records. The default is 500.
          type: integer
          minimum: 100
          maximum: 20000
          example: "500"
        producer_batch_size:
          title: producer.batch.size
          description: The batch size in bytes producer will attempt to collect before publishing to broker.
          type: integer
          minimum: 0
          maximum: 5.24288e+06
          example: "1024"
        producer_buffer_memory:
          title: producer.buffer.memory
          description: The amount of bytes producer can use for buffering data before publishing to broker.
          type: integer
          minimum: 5.24288e+06
          maximum: 1.34217728e+08
          example: "8388608"
        producer_compression_type:
          title: producer.compression.type
          description: Specify the default compression type for producers. This configuration accepts the standard compression codecs ('gzip', 'snappy', 'lz4', 'zstd'). It additionally accepts 'none' which is the default and equivalent to no compression.
          type: string
          enum:
            - value: gzip
            - value: lz4
            - value: none
            - value: snappy
            - value: zstd
        producer_linger_ms:
          title: producer.linger.ms
          description: The linger time (ms) for waiting new data to arrive for publishing.
          type: integer
          minimum: 0
          maximum: 5000
          example: "100"
        producer_max_request_size:
          title: producer.max.request.size
          description: The maximum request size in bytes.
          type: integer
          minimum: 0
          maximum: 2.68435456e+08
          example: "1048576"
logs:
  type: object
  properties:
    elasticsearch_index_days_max:
      title: Elasticsearch index retention limit
      type: integer
      default: "3"
      minimum: 1
      maximum: 10000
      example: "5"
    elasticsearch_index_prefix:
      title: Elasticsearch index prefix
      type: string
      default: logs
      min_length: 1
      max_length: 1024
      pattern: ^[a-z0-9][a-z0-9-_.]+$
      example: logs
      user_error: Must start with alpha-numeric character, and only contain alpha-numeric characters, dashes, underscores and dots
    selected_log_fields:
      title: List of logging fields to include
      description: The list of logging fields that will be sent to the integration logging service. The MESSAGE and timestamp fields are always sent.
      type: array
      items:
        title: Log field name
        type: string
        enum:
          - value: HOSTNAME
          - value: PRIORITY
          - value: REALTIME_TIMESTAMP
          - value: SYSTEMD_UNIT
          - value: service_name
      max_items: 5
m3aggregator:
  is_deprecated: true
  deprecation_notice: This property is deprecated.
  title: Integration user config
  type: object
m3coordinator:
  is_deprecated: true
  deprecation_notice: This property is deprecated.
  title: Integration user config
  type: object
metrics:
  title: Integration user config
  type: object
  properties:
    database:
      title: Name of the database where to store metric datapoints. Only affects PostgreSQL destinations. Defaults to 'metrics'. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
      type: string
      max_length: 40
      pattern: ^[_A-Za-z0-9][-_A-Za-z0-9]{0,39}$
      example: metrics
      user_error: Must consist of alpha-numeric characters, underscores or dashes, may not start with dash, max 40 characters
    retention_days:
      title: Number of days to keep old metrics. Only affects PostgreSQL destinations. Set to 0 for no automatic cleanup. Defaults to 30 days.
      type: integer
      minimum: 0
      maximum: 10000
      example: "30"
    ro_username:
      title: Name of a user that can be used to read metrics. This will be used for Grafana integration (if enabled) to prevent Grafana users from making undesired changes. Only affects PostgreSQL destinations. Defaults to 'metrics_reader'. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
      type: string
      max_length: 40
      pattern: ^[_A-Za-z0-9][-._A-Za-z0-9]{0,39}$
      example: metrics_reader
      user_error: Must consist of alpha-numeric characters, dots, underscores or dashes, may not start with dash or dot, max 40 characters
    source_mysql:
      title: Configuration options for metrics where source service is MySQL
      type: object
      properties:
        telegraf:
          title: Configuration options for Telegraf MySQL input plugin
          type: object
          properties:
            gather_event_waits:
              title: Gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS
              type: boolean
              example: false
            gather_file_events_stats:
              title: gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME
              type: boolean
              example: false
            gather_index_io_waits:
              title: Gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE
              type: boolean
              example: false
            gather_info_schema_auto_inc:
              title: Gather auto_increment columns and max values from information schema
              type: boolean
              example: false
            gather_innodb_metrics:
              title: Gather metrics from INFORMATION_SCHEMA.INNODB_METRICS
              type: boolean
              example: true
            gather_perf_events_statements:
              title: Gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST
              type: boolean
              example: false
            gather_process_list:
              title: Gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST
              type: boolean
              example: true
            gather_slave_status:
              title: Gather metrics from SHOW SLAVE STATUS command output
              type: boolean
              example: true
            gather_table_io_waits:
              title: Gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE
              type: boolean
              example: false
            gather_table_lock_waits:
              title: Gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS
              type: boolean
              example: false
            gather_table_schema:
              title: Gather metrics from INFORMATION_SCHEMA.TABLES
              type: boolean
              example: true
            perf_events_statements_digest_text_limit:
              title: Truncates digest text from perf_events_statements into this many characters
              type: integer
              minimum: 1
              maximum: 2048
              example: "120"
            perf_events_statements_limit:
              title: Limits metrics from perf_events_statements
              type: integer
              minimum: 1
              maximum: 4000
              example: "250"
            perf_events_statements_time_limit:
              title: Only include perf_events_statements whose last seen is less than this many seconds
              type: integer
              minimum: 1
              maximum: 2.592e+06
              example: "86400"
    username:
      title: Name of the user used to write metrics. Only affects PostgreSQL destinations. Defaults to 'metrics_writer'. Note that this must be the same for all metrics integrations that write data to the same PostgreSQL service.
      type: string
      max_length: 40
      pattern: ^[_A-Za-z0-9][-._A-Za-z0-9]{0,39}$
      example: metrics_writer
      user_error: Must consist of alpha-numeric characters, dots, underscores or dashes, may not start with dash or dot, max 40 characters
prometheus:
  title: Integration user config
  type: object
  properties:
    source_mysql:
      title: Configuration options for metrics where source service is MySQL
      type: object
      properties:
        telegraf:
          title: Configuration options for Telegraf MySQL input plugin
          type: object
          properties:
            gather_event_waits:
              title: Gather metrics from PERFORMANCE_SCHEMA.EVENT_WAITS
              type: boolean
              example: false
            gather_file_events_stats:
              title: gather metrics from PERFORMANCE_SCHEMA.FILE_SUMMARY_BY_EVENT_NAME
              type: boolean
              example: false
            gather_index_io_waits:
              title: Gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_INDEX_USAGE
              type: boolean
              example: false
            gather_info_schema_auto_inc:
              title: Gather auto_increment columns and max values from information schema
              type: boolean
              example: false
            gather_innodb_metrics:
              title: Gather metrics from INFORMATION_SCHEMA.INNODB_METRICS
              type: boolean
              example: true
            gather_perf_events_statements:
              title: Gather metrics from PERFORMANCE_SCHEMA.EVENTS_STATEMENTS_SUMMARY_BY_DIGEST
              type: boolean
              example: false
            gather_process_list:
              title: Gather thread state counts from INFORMATION_SCHEMA.PROCESSLIST
              type: boolean
              example: true
            gather_slave_status:
              title: Gather metrics from SHOW SLAVE STATUS command output
              type: boolean
              example: true
            gather_table_io_waits:
              title: Gather metrics from PERFORMANCE_SCHEMA.TABLE_IO_WAITS_SUMMARY_BY_TABLE
              type: boolean
              example: false
            gather_table_lock_waits:
              title: Gather metrics from PERFORMANCE_SCHEMA.TABLE_LOCK_WAITS
              type: boolean
              example: false
            gather_table_schema:
              title: Gather metrics from INFORMATION_SCHEMA.TABLES
              type: boolean
              example: true
            perf_events_statements_digest_text_limit:
              title: Truncates digest text from perf_events_statements into this many characters
              type: integer
              minimum: 1
              maximum: 2048
              example: "120"
            perf_events_statements_limit:
              title: Limits metrics from perf_events_statements
              type: integer
              minimum: 1
              maximum: 4000
              example: "250"
            perf_events_statements_time_limit:
              title: Only include perf_events_statements whose last seen is less than this many seconds
              type: integer
              minimum: 1
              maximum: 2.592e+06
              example: "86400"
rsyslog:
  title: Integration user config
  type: object
thanos_migrate:
  is_deprecated: true
  deprecation_notice: This property is deprecated.
  title: Integration user config
  type: object
